{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-24T19:40:00.430328Z","iopub.execute_input":"2022-04-24T19:40:00.431113Z","iopub.status.idle":"2022-04-24T19:40:00.442160Z","shell.execute_reply.started":"2022-04-24T19:40:00.431078Z","shell.execute_reply":"2022-04-24T19:40:00.441208Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"train_path = os.path.join(\"../input/learn-ai-bbc/\", \"BBC News Train.csv\")\ntest_path = os.path.join(\"../input/learn-ai-bbc/\", \"BBC News Test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-04-24T19:40:01.872617Z","iopub.execute_input":"2022-04-24T19:40:01.873065Z","iopub.status.idle":"2022-04-24T19:40:01.876733Z","shell.execute_reply.started":"2022-04-24T19:40:01.873007Z","shell.execute_reply":"2022-04-24T19:40:01.876166Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(train_path)\ntest_df = pd.read_csv(test_path)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T19:40:03.998171Z","iopub.execute_input":"2022-04-24T19:40:03.998945Z","iopub.status.idle":"2022-04-24T19:40:04.079704Z","shell.execute_reply.started":"2022-04-24T19:40:03.998889Z","shell.execute_reply":"2022-04-24T19:40:04.079122Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at what the train and test dataset look like:","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T19:40:05.147087Z","iopub.execute_input":"2022-04-24T19:40:05.147737Z","iopub.status.idle":"2022-04-24T19:40:05.158680Z","shell.execute_reply.started":"2022-04-24T19:40:05.147705Z","shell.execute_reply":"2022-04-24T19:40:05.157927Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T19:40:07.983786Z","iopub.execute_input":"2022-04-24T19:40:07.984248Z","iopub.status.idle":"2022-04-24T19:40:07.993123Z","shell.execute_reply.started":"2022-04-24T19:40:07.984199Z","shell.execute_reply":"2022-04-24T19:40:07.992457Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T19:40:09.189492Z","iopub.execute_input":"2022-04-24T19:40:09.190270Z","iopub.status.idle":"2022-04-24T19:40:09.205552Z","shell.execute_reply.started":"2022-04-24T19:40:09.190231Z","shell.execute_reply":"2022-04-24T19:40:09.204427Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"train_df.Text[0]","metadata":{"execution":{"iopub.status.busy":"2022-04-24T19:40:10.222331Z","iopub.execute_input":"2022-04-24T19:40:10.222640Z","iopub.status.idle":"2022-04-24T19:40:10.228167Z","shell.execute_reply.started":"2022-04-24T19:40:10.222603Z","shell.execute_reply":"2022-04-24T19:40:10.227431Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"You can see that there are potential problems in the text. Each cell of text is a single long string. A lot of word spacing induced because a lot of punctuation was removed. Not all punctuation was removed and you have things like dollar signs. TfidfVectorizer will do a lot of the work for us with stopwords so I think it'll likely be fine.","metadata":{}},{"cell_type":"code","source":"train_df.Category.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T19:40:11.297123Z","iopub.execute_input":"2022-04-24T19:40:11.297908Z","iopub.status.idle":"2022-04-24T19:40:11.306221Z","shell.execute_reply.started":"2022-04-24T19:40:11.297855Z","shell.execute_reply":"2022-04-24T19:40:11.305353Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"train_df.Category.value_counts()/train_df.Category.value_counts().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T19:40:12.268807Z","iopub.execute_input":"2022-04-24T19:40:12.269196Z","iopub.status.idle":"2022-04-24T19:40:12.276968Z","shell.execute_reply.started":"2022-04-24T19:40:12.269167Z","shell.execute_reply":"2022-04-24T19:40:12.276432Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"There's a good split of everything with sports and business having the most amount of datat but not terribly lopsided with a singlular catgeory below 5% or anything like that. There are nearly 1500 rows so every category should have a good size and we can see below that there aren't any missing labels either.","metadata":{}},{"cell_type":"code","source":"train_df.Category.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-24T19:40:13.272620Z","iopub.execute_input":"2022-04-24T19:40:13.273101Z","iopub.status.idle":"2022-04-24T19:40:13.283011Z","shell.execute_reply.started":"2022-04-24T19:40:13.273056Z","shell.execute_reply":"2022-04-24T19:40:13.282372Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import NMF\n","metadata":{"execution":{"iopub.status.busy":"2022-04-24T19:40:14.254008Z","iopub.execute_input":"2022-04-24T19:40:14.254650Z","iopub.status.idle":"2022-04-24T19:40:14.258291Z","shell.execute_reply.started":"2022-04-24T19:40:14.254614Z","shell.execute_reply":"2022-04-24T19:40:14.257142Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"We're gonna set min word count to 20. Previously I tried 5 which gave more accurate results on the training data, but it had more features than what was provided in the test set. I think this is because there are words that ended being used as features in train, but did not appear often enough in test and it creates a dimensionality error. For NMF, n-components we have to pick 5 because we know there are 5 labels.","metadata":{}},{"cell_type":"code","source":"vectorized = TfidfVectorizer(stop_words='english', min_df=20, sublinear_tf=True)\nX = vectorized.fit_transform(train_df.Text)\nmodel = NMF(n_components=5, random_state=1234)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:44:14.083725Z","iopub.execute_input":"2022-04-24T20:44:14.084031Z","iopub.status.idle":"2022-04-24T20:44:14.486221Z","shell.execute_reply.started":"2022-04-24T20:44:14.083997Z","shell.execute_reply":"2022-04-24T20:44:14.485608Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"model.fit(X)\npred = model.transform(X)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:44:17.608420Z","iopub.execute_input":"2022-04-24T20:44:17.608732Z","iopub.status.idle":"2022-04-24T20:44:17.841922Z","shell.execute_reply.started":"2022-04-24T20:44:17.608700Z","shell.execute_reply":"2022-04-24T20:44:17.840800Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"pred[0:6]","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:44:18.818281Z","iopub.execute_input":"2022-04-24T20:44:18.819020Z","iopub.status.idle":"2022-04-24T20:44:18.825850Z","shell.execute_reply.started":"2022-04-24T20:44:18.818984Z","shell.execute_reply":"2022-04-24T20:44:18.824896Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"markdown","source":"Index of highest value in each row is the predicted label category.","metadata":{}},{"cell_type":"code","source":"preds = pred.argmax(1)\npreds[0:6]","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:44:19.769842Z","iopub.execute_input":"2022-04-24T20:44:19.770253Z","iopub.status.idle":"2022-04-24T20:44:19.776533Z","shell.execute_reply.started":"2022-04-24T20:44:19.770217Z","shell.execute_reply":"2022-04-24T20:44:19.775916Z"},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"markdown","source":"Find highest word values in each component to determine label from integer class.","metadata":{}},{"cell_type":"code","source":"components_df = pd.DataFrame(model.components_, columns=vectorized.get_feature_names())\ncomponents_df","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:44:20.839423Z","iopub.execute_input":"2022-04-24T20:44:20.839862Z","iopub.status.idle":"2022-04-24T20:44:20.866265Z","shell.execute_reply.started":"2022-04-24T20:44:20.839825Z","shell.execute_reply":"2022-04-24T20:44:20.865712Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"code","source":"for cat in range(components_df.shape[0]):\n    tmp = components_df.iloc[cat]\n    print(f'For category {cat} the words with the highest value are:')\n    print(tmp.nlargest(6))\n    print('\\n')","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:44:22.075166Z","iopub.execute_input":"2022-04-24T20:44:22.075838Z","iopub.status.idle":"2022-04-24T20:44:22.091051Z","shell.execute_reply.started":"2022-04-24T20:44:22.075797Z","shell.execute_reply":"2022-04-24T20:44:22.089981Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"lab_dict = {0:'tech', 1:'sport', 2:'politics', 3:'entertainment', 4:'business'}\nlabels = np.vectorize(lab_dict.get)(preds) \nlabels[0:6]","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:44:44.080067Z","iopub.execute_input":"2022-04-24T20:44:44.080387Z","iopub.status.idle":"2022-04-24T20:44:44.088384Z","shell.execute_reply.started":"2022-04-24T20:44:44.080355Z","shell.execute_reply":"2022-04-24T20:44:44.087534Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"acc_NMF = np.sum(labels == train_df.Category)/len(labels)\nacc_NMF","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:44:44.882038Z","iopub.execute_input":"2022-04-24T20:44:44.882494Z","iopub.status.idle":"2022-04-24T20:44:44.890663Z","shell.execute_reply.started":"2022-04-24T20:44:44.882446Z","shell.execute_reply":"2022-04-24T20:44:44.889728Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"code","source":"X_test = vectorized.transform(test_df.Text)\nnmf_pred_test = model.transform(X_test)\nnmf_preds_test = nmf_pred_test.argmax(1)\nnmf_labels_test = np.vectorize(lab_dict.get)(nmf_preds_test) \npd.DataFrame(data={'ArticleId':test_df.ArticleId, 'Category':nmf_labels_test}).to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:44:45.923906Z","iopub.execute_input":"2022-04-24T20:44:45.924356Z","iopub.status.idle":"2022-04-24T20:44:46.129295Z","shell.execute_reply.started":"2022-04-24T20:44:45.924311Z","shell.execute_reply":"2022-04-24T20:44:46.128416Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"markdown","source":"NMF Test Score: 0.93877","metadata":{}},{"cell_type":"markdown","source":"Now we're gonna compare the result to a default sklearn KNN using KNeighborsClassifier. We'll have to reverse the labels into integers and then fit to vectorized data.","metadata":{}},{"cell_type":"code","source":"reverse_dict = {v:k for k,v in lab_dict.items()}\nreverse_dict","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:44:47.450953Z","iopub.execute_input":"2022-04-24T20:44:47.451234Z","iopub.status.idle":"2022-04-24T20:44:47.457202Z","shell.execute_reply.started":"2022-04-24T20:44:47.451203Z","shell.execute_reply":"2022-04-24T20:44:47.456366Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"y = np.vectorize(reverse_dict.get)(train_df.Category)\ny","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:44:55.199163Z","iopub.execute_input":"2022-04-24T20:44:55.199432Z","iopub.status.idle":"2022-04-24T20:44:55.204825Z","shell.execute_reply.started":"2022-04-24T20:44:55.199403Z","shell.execute_reply":"2022-04-24T20:44:55.203979Z"},"trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn_model = KNeighborsClassifier()\nknn_model.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:44:56.107553Z","iopub.execute_input":"2022-04-24T20:44:56.107852Z","iopub.status.idle":"2022-04-24T20:44:56.116928Z","shell.execute_reply.started":"2022-04-24T20:44:56.107802Z","shell.execute_reply":"2022-04-24T20:44:56.115850Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"code","source":"knn_preds = knn_model.predict(X)\nknn_labels = np.vectorize(lab_dict.get)(knn_preds) ","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:44:57.322118Z","iopub.execute_input":"2022-04-24T20:44:57.322941Z","iopub.status.idle":"2022-04-24T20:44:57.534760Z","shell.execute_reply.started":"2022-04-24T20:44:57.322886Z","shell.execute_reply":"2022-04-24T20:44:57.534073Z"},"trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"code","source":"acc_knn = np.sum(knn_labels == train_df.Category)/len(knn_labels)\nacc_knn","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:44:58.546591Z","iopub.execute_input":"2022-04-24T20:44:58.547046Z","iopub.status.idle":"2022-04-24T20:44:58.554319Z","shell.execute_reply.started":"2022-04-24T20:44:58.547000Z","shell.execute_reply":"2022-04-24T20:44:58.553529Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"X_test = vectorized.transform(test_df.Text)\nknn_preds_test = knn_model.predict(X_test)\nknn_labels_test = np.vectorize(lab_dict.get)(knn_preds_test) ","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:44:59.413073Z","iopub.execute_input":"2022-04-24T20:44:59.413522Z","iopub.status.idle":"2022-04-24T20:44:59.699031Z","shell.execute_reply.started":"2022-04-24T20:44:59.413477Z","shell.execute_reply":"2022-04-24T20:44:59.698444Z"},"trusted":true},"execution_count":141,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(data={'ArticleId':test_df.ArticleId, 'Category':knn_labels_test}).to_csv('submission2.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-04-24T20:47:58.139350Z","iopub.execute_input":"2022-04-24T20:47:58.140021Z","iopub.status.idle":"2022-04-24T20:47:58.148061Z","shell.execute_reply.started":"2022-04-24T20:47:58.139972Z","shell.execute_reply":"2022-04-24T20:47:58.147468Z"},"trusted":true},"execution_count":143,"outputs":[]},{"cell_type":"markdown","source":"KNN Test Score: 0.96190","metadata":{}},{"cell_type":"markdown","source":"| model | train_accuracy | test_accuracy\n|----|-----|------|\n|NMF|0.9369|0.93877|\n|KNN|0.9758|0.96190|","metadata":{}},{"cell_type":"markdown","source":"Assuming this is done right and on data already seen, NMF actually does worse than default KNN. Since default KNN uses 5 neighbors, it is very likely to be overfitting. It really seems that TfidfVectorizer is doing all the heavy lifting. Now on unseen data with unseen text words, NMF score slightly improved, but that while KNN slightly dipped, but still seems reasonably better than NMF. Perhaps a larger K for neighbors would increase accuracy since it may be overfitting. There's submission limitations and the test data provided does not have category labels provided so it would be difficult to do some parameter searching without splitting up the train set further to allow for validation.","metadata":{}},{"cell_type":"markdown","source":"Citation: https://predictivehacks.com/topic-modelling-with-nmf-in-python/","metadata":{}}]}