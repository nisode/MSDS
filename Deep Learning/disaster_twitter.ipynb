{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-16T10:46:39.890437Z","iopub.execute_input":"2022-08-16T10:46:39.890951Z","iopub.status.idle":"2022-08-16T10:46:39.909234Z","shell.execute_reply.started":"2022-08-16T10:46:39.890898Z","shell.execute_reply":"2022-08-16T10:46:39.906297Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"https://github.com/nisode/MSDS/tree/main/Deep%20Learning","metadata":{}},{"cell_type":"markdown","source":"# Task","metadata":{}},{"cell_type":"markdown","source":"The goal is to categorize Twitter posts as disaster tweets or not.\n\nTwitter posts are fast and can be useful to be monitored to spot the beginnings of a disaster.\n\nWe are given about 7.6k tweets to train our data and then to categorize on an unseen test set.\n\nSome posts have misleading texts which can result in false flags.","metadata":{}},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/nlp-getting-started/train.csv')\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:46:42.956525Z","iopub.execute_input":"2022-08-16T10:46:42.957059Z","iopub.status.idle":"2022-08-16T10:46:43.028997Z","shell.execute_reply.started":"2022-08-16T10:46:42.957014Z","shell.execute_reply":"2022-08-16T10:46:43.027865Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Positive target (1) means a disaster related tweet.","metadata":{}},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"Let's see what our data looks like and data types are:","metadata":{}},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:46:44.710324Z","iopub.execute_input":"2022-08-16T10:46:44.710762Z","iopub.status.idle":"2022-08-16T10:46:44.740109Z","shell.execute_reply.started":"2022-08-16T10:46:44.710725Z","shell.execute_reply":"2022-08-16T10:46:44.739140Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_df.isnull().sum().plot(kind='bar', color=['red', 'blue'])","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:46:46.042946Z","iopub.execute_input":"2022-08-16T10:46:46.043503Z","iopub.status.idle":"2022-08-16T10:46:46.286984Z","shell.execute_reply.started":"2022-08-16T10:46:46.043459Z","shell.execute_reply":"2022-08-16T10:46:46.286046Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Checking our data columns for missing values, location has around a third of its data missing. That would make it hard for categorization so I think we won't consider using them. Keyword has a few missing as well.","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv('../input/nlp-getting-started/test.csv')\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:46:47.132166Z","iopub.execute_input":"2022-08-16T10:46:47.132747Z","iopub.status.idle":"2022-08-16T10:46:47.163690Z","shell.execute_reply.started":"2022-08-16T10:46:47.132713Z","shell.execute_reply":"2022-08-16T10:46:47.162184Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"test_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:46:48.169186Z","iopub.execute_input":"2022-08-16T10:46:48.169556Z","iopub.status.idle":"2022-08-16T10:46:48.183321Z","shell.execute_reply.started":"2022-08-16T10:46:48.169525Z","shell.execute_reply":"2022-08-16T10:46:48.182127Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"test_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:46:49.091045Z","iopub.execute_input":"2022-08-16T10:46:49.091745Z","iopub.status.idle":"2022-08-16T10:46:49.101326Z","shell.execute_reply.started":"2022-08-16T10:46:49.091708Z","shell.execute_reply":"2022-08-16T10:46:49.100213Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"In the test data, location also has about a third of its data missing. I think I will be dropping this and keyword because I'm not sure how I would include either in a matrix of tokens.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:46:50.032200Z","iopub.execute_input":"2022-08-16T10:46:50.032864Z","iopub.status.idle":"2022-08-16T10:46:50.037936Z","shell.execute_reply.started":"2022-08-16T10:46:50.032829Z","shell.execute_reply":"2022-08-16T10:46:50.036738Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at the target value breakdown:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(6,6))\ntrain_df['target'].value_counts(normalize=True).plot(kind=\"pie\", autopct='%1.1f%%')","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:46:51.167864Z","iopub.execute_input":"2022-08-16T10:46:51.168453Z","iopub.status.idle":"2022-08-16T10:46:51.294619Z","shell.execute_reply.started":"2022-08-16T10:46:51.168408Z","shell.execute_reply":"2022-08-16T10:46:51.293595Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_df['target'].value_counts().plot(kind='barh', color=['blue','red'])","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:46:52.220082Z","iopub.execute_input":"2022-08-16T10:46:52.220778Z","iopub.status.idle":"2022-08-16T10:46:52.379330Z","shell.execute_reply.started":"2022-08-16T10:46:52.220743Z","shell.execute_reply":"2022-08-16T10:46:52.378088Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"About 57% not disaster and 43% disaster. It is a bit imbalance, but I won't worry about it this time.","metadata":{}},{"cell_type":"code","source":"train_df['keyword'].value_counts()[0:10]","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:46:53.139505Z","iopub.execute_input":"2022-08-16T10:46:53.139878Z","iopub.status.idle":"2022-08-16T10:46:53.149629Z","shell.execute_reply.started":"2022-08-16T10:46:53.139847Z","shell.execute_reply":"2022-08-16T10:46:53.148073Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Just a look at what the keywords are and how they might be used in another attempt.","metadata":{}},{"cell_type":"markdown","source":"# Cleaning Data","metadata":{}},{"cell_type":"markdown","source":"I see a lot of notebooks use this preprocessing pipeline or something very similar, so it must be popular and effective solution to this problem. I have one of the notebooks where I found it linked in the comment section below.\n\nIt goes through a 9 step procress:\n\n1. removes any @ tagging\n2. removes any digits\n3. removes any hastag symbols\n4. removes links such as emails and websites\n5. removes anything that doesn't start with a character\n6. lower cases everything\n7. removes any stopwords\n8. lemmatize which turns words to their base words\n9. joins it all back together","metadata":{}},{"cell_type":"code","source":"# preprocessPipeline base from here: https://www.kaggle.com/code/alid3bs/lstm-vs-gru-vs-bidirectional/notebook\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\nimport nltk\nnltk.download('omw-1.4')\n\nlemmatizer = WordNetLemmatizer()\nstop_words = stopwords.words('english')\n\ndef preprocessPipline(data, labelName):\n    \n    dataTemp = data.copy()\n    dataTemp[labelName] = dataTemp[labelName].apply(lambda x: re.sub('(\\s*)@\\w+(\\s*)','', x))\n    dataTemp[labelName] = dataTemp[labelName].apply(lambda x: re.sub('\\d+','', x))\n    dataTemp[labelName] = dataTemp[labelName].apply(lambda x: re.sub('#','', x))\n    dataTemp[labelName] = dataTemp[labelName].apply(lambda x: re.sub('https?://\\S+|www\\.\\S+','',x))\n    dataTemp[labelName] = dataTemp[labelName].apply(lambda x: re.sub('[^A-Za-z]',' ',x))\n    dataTemp[labelName] = dataTemp[labelName].apply(lambda x: x.lower())\n    dataTemp[labelName] = dataTemp[labelName].apply(lambda x : [word for word in x.split()  if word not in stop_words])\n    dataTemp[labelName] = dataTemp[labelName].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n    dataTemp[labelName] = dataTemp[labelName].apply(lambda x: ' '.join(x))\n    return dataTemp","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:46:54.882134Z","iopub.execute_input":"2022-08-16T10:46:54.882731Z","iopub.status.idle":"2022-08-16T10:46:56.132995Z","shell.execute_reply.started":"2022-08-16T10:46:54.882697Z","shell.execute_reply":"2022-08-16T10:46:56.132047Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train_df_cleaned = preprocessPipline(train_df, \"text\")\nclean_text = train_df_cleaned['text']\nclean_text[0:5]","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:46:56.643071Z","iopub.execute_input":"2022-08-16T10:46:56.644091Z","iopub.status.idle":"2022-08-16T10:46:58.927111Z","shell.execute_reply.started":"2022-08-16T10:46:56.644042Z","shell.execute_reply":"2022-08-16T10:46:58.926164Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Preparing Data","metadata":{}},{"cell_type":"markdown","source":"Using tokenizer, we will turn the token counts into integer sequences to represent the order of words in the twitter posts with respect to the token in the tokenizer dictionary. Then padding makes it so all the sequences are the same length as the longest one.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntokenizer = Tokenizer(oov_token=\"unk\")\ntokenizer.fit_on_texts(clean_text)\nX = tokenizer.texts_to_sequences(clean_text)\n\nX = pad_sequences(X, padding='post',truncating='post')\nX","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:46:58.929046Z","iopub.execute_input":"2022-08-16T10:46:58.929419Z","iopub.status.idle":"2022-08-16T10:47:04.402735Z","shell.execute_reply.started":"2022-08-16T10:46:58.929375Z","shell.execute_reply":"2022-08-16T10:47:04.401800Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:47:04.404597Z","iopub.execute_input":"2022-08-16T10:47:04.405221Z","iopub.status.idle":"2022-08-16T10:47:04.411999Z","shell.execute_reply.started":"2022-08-16T10:47:04.405188Z","shell.execute_reply":"2022-08-16T10:47:04.410926Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"We can see the longest post (minus stopwords) is 23 tokens long.","metadata":{}},{"cell_type":"markdown","source":"We put the targets as their own series and split the data into train and validation set by 0.2 and keeping a proportional amount of Y.","metadata":{}},{"cell_type":"code","source":"Y = train_df_cleaned['target']\nY ","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:47:04.413834Z","iopub.execute_input":"2022-08-16T10:47:04.414551Z","iopub.status.idle":"2022-08-16T10:47:04.430222Z","shell.execute_reply.started":"2022-08-16T10:47:04.414516Z","shell.execute_reply":"2022-08-16T10:47:04.429127Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_val ,y_train ,y_val = train_test_split(X, Y, test_size=0.2, stratify=Y)\nx_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:47:04.432648Z","iopub.execute_input":"2022-08-16T10:47:04.433006Z","iopub.status.idle":"2022-08-16T10:47:04.445945Z","shell.execute_reply.started":"2022-08-16T10:47:04.432970Z","shell.execute_reply":"2022-08-16T10:47:04.444814Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(tokenizer.word_index)\nvocab_size","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:47:05.750355Z","iopub.execute_input":"2022-08-16T10:47:05.750981Z","iopub.status.idle":"2022-08-16T10:47:05.758475Z","shell.execute_reply.started":"2022-08-16T10:47:05.750945Z","shell.execute_reply":"2022-08-16T10:47:05.757201Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Model Architecture","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Dropout, Flatten, Conv2D, MaxPooling2D, Dense, Activation\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:47:09.670750Z","iopub.execute_input":"2022-08-16T10:47:09.671656Z","iopub.status.idle":"2022-08-16T10:47:09.681030Z","shell.execute_reply.started":"2022-08-16T10:47:09.671607Z","shell.execute_reply":"2022-08-16T10:47:09.680021Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Some explanations for architecture choices:\n1. embedding apparently needs vocab_size + 1 according to keras documentation\n2. relus seem to work good previously, so I will be reusing them\n3. sigmoid with 1 output in the output layer because it is a binary classification problem\n3. Adam seems to work well in general so this will also be reused.\n4. I will be tuning number of hidden layers, drop out thresholds, and learning rates\n6. Will try versions where lstm is bidirectional and versions where it is not\n7. I pick LSTM for this problem, because sentences can be seen as time series data and it can help us deal with any vanishing gradient problems","metadata":{}},{"cell_type":"code","source":"lstm_model1 = Sequential([\n    layers.Embedding(vocab_size + 1,64),\n    Dropout(0.2),\n    layers.LSTM(64),\n    Dense(23, activation = 'relu'),\n    Dropout(0.2),\n    Dense(23, activation = 'relu'),\n    Dense(1, activation = 'sigmoid')\n])\n\nlstm_model1.compile(optimizer = Adam(learning_rate=0.001),\n                    loss=\"binary_crossentropy\", \n                    metrics = ['accuracy', tf.keras.metrics.AUC()])\n\nlstm_model1.summary()","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:47:24.834011Z","iopub.execute_input":"2022-08-16T10:47:24.835057Z","iopub.status.idle":"2022-08-16T10:47:28.006387Z","shell.execute_reply.started":"2022-08-16T10:47:24.835008Z","shell.execute_reply":"2022-08-16T10:47:28.005420Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# Results","metadata":{}},{"cell_type":"code","source":"lstm_model1_hist = lstm_model1.fit(x = x_train,\n                                   y = y_train,\n                                   validation_data = (x_val, y_val),\n                                   epochs = 10,\n                                   verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:49:30.571990Z","iopub.execute_input":"2022-08-16T10:49:30.572386Z","iopub.status.idle":"2022-08-16T10:49:52.984368Z","shell.execute_reply.started":"2022-08-16T10:49:30.572353Z","shell.execute_reply":"2022-08-16T10:49:52.983282Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def metrics_plot(history):\n    xs = np.arange(1, len(history['loss'])+1)\n    plt.figure(figsize=[16,4])\n    \n    plt.subplot(1,3,1)\n    plt.plot(xs, history['loss'], label='Training')\n    plt.plot(xs, history['val_loss'], label='Validation')\n    plt.xlabel('Epoch') \n    plt.ylabel('Loss') \n    plt.title('Loss')\n    plt.legend()\n    \n    plt.subplot(1,3,2)\n    plt.plot(xs, history['accuracy'], label='Training')\n    plt.plot(xs, history['val_accuracy'], label='Validation')\n    plt.xlabel('Epoch') \n    plt.ylabel('Accuracy')\n    plt.title('Accuracy')\n    plt.legend()\n    \n    plt.subplot(1,3,3)\n    plt.plot(xs, history[list(history.keys())[2]], label='Training')\n    plt.plot(xs, history[list(history.keys())[5]], label='Validation')\n    plt.xlabel('Epoch') \n    plt.ylabel('AUC') \n    plt.title('AUC')\n    plt.legend()\n    plt.tight_layout()\n    \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:53:11.407231Z","iopub.execute_input":"2022-08-16T10:53:11.407788Z","iopub.status.idle":"2022-08-16T10:53:11.419802Z","shell.execute_reply.started":"2022-08-16T10:53:11.407752Z","shell.execute_reply":"2022-08-16T10:53:11.418836Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"metrics_plot(lstm_model1_hist.history)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:53:27.982251Z","iopub.execute_input":"2022-08-16T10:53:27.983245Z","iopub.status.idle":"2022-08-16T10:53:28.499173Z","shell.execute_reply.started":"2022-08-16T10:53:27.983207Z","shell.execute_reply":"2022-08-16T10:53:28.496467Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"lstm_model2 = Sequential([\n    layers.Embedding(vocab_size + 1,64),\n    Dropout(0.5),\n    layers.LSTM(64),\n    Dense(23, activation = 'relu'),\n    Dropout(0.5),\n    Dense(23, activation = 'relu'),\n    Dense(1, activation = 'sigmoid')\n])\n\nlstm_model2.compile(optimizer = Adam(learning_rate=0.001),\n                    loss=\"binary_crossentropy\", \n                    metrics = ['accuracy', tf.keras.metrics.AUC()])\n\nlstm_model2.summary()","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:50:42.112124Z","iopub.execute_input":"2022-08-16T10:50:42.112670Z","iopub.status.idle":"2022-08-16T10:50:42.401892Z","shell.execute_reply.started":"2022-08-16T10:50:42.112635Z","shell.execute_reply":"2022-08-16T10:50:42.400791Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"lstm_model2_hist = lstm_model2.fit(x = x_train,\n                                   y = y_train,\n                                   validation_data = (x_val, y_val),\n                                   epochs = 10,\n                                   verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:51:05.569814Z","iopub.execute_input":"2022-08-16T10:51:05.570204Z","iopub.status.idle":"2022-08-16T10:51:20.629303Z","shell.execute_reply.started":"2022-08-16T10:51:05.570167Z","shell.execute_reply":"2022-08-16T10:51:20.628187Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"metrics_plot(lstm_model2_hist.history)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:54:06.913349Z","iopub.execute_input":"2022-08-16T10:54:06.914005Z","iopub.status.idle":"2022-08-16T10:54:07.364520Z","shell.execute_reply.started":"2022-08-16T10:54:06.913968Z","shell.execute_reply":"2022-08-16T10:54:07.363483Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"lstm_model3 = Sequential([\n    layers.Embedding(vocab_size + 1,64),\n    Dropout(0.5),\n    layers.LSTM(64),\n    Dense(23, activation = 'relu'),\n    Dropout(0.5),\n    Dense(23, activation = 'relu'),\n    Dense(1, activation = 'sigmoid')\n])\n\nlstm_model3.compile(optimizer = Adam(learning_rate=0.0001),\n                    loss=\"binary_crossentropy\", \n                    metrics = ['accuracy', tf.keras.metrics.AUC()])\n\nlstm_model3.summary()","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:55:08.251825Z","iopub.execute_input":"2022-08-16T10:55:08.252212Z","iopub.status.idle":"2022-08-16T10:55:08.491831Z","shell.execute_reply.started":"2022-08-16T10:55:08.252178Z","shell.execute_reply":"2022-08-16T10:55:08.490128Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"lstm_model3_hist = lstm_model3.fit(x = x_train,\n                                   y = y_train,\n                                   validation_data = (x_val, y_val),\n                                   epochs = 10,\n                                   verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:55:22.810706Z","iopub.execute_input":"2022-08-16T10:55:22.811919Z","iopub.status.idle":"2022-08-16T10:55:38.134281Z","shell.execute_reply.started":"2022-08-16T10:55:22.811871Z","shell.execute_reply":"2022-08-16T10:55:38.133328Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"metrics_plot(lstm_model3_hist.history)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T10:55:38.136522Z","iopub.execute_input":"2022-08-16T10:55:38.136922Z","iopub.status.idle":"2022-08-16T10:55:38.600069Z","shell.execute_reply.started":"2022-08-16T10:55:38.136884Z","shell.execute_reply":"2022-08-16T10:55:38.599104Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"lstm_model4 = Sequential([\n    layers.Embedding(vocab_size + 1,64),\n    Dropout(0.2),\n    layers.LSTM(64),\n    Dense(23, activation = 'relu'),\n    Dropout(0.2),\n    Dense(23, activation = 'relu'),\n    Dense(1, activation = 'sigmoid')\n])\n\nlstm_model4.compile(optimizer = Adam(learning_rate=0.0001),\n                    loss=\"binary_crossentropy\", \n                    metrics = ['accuracy', tf.keras.metrics.AUC()])\n\nlstm_model4.summary()","metadata":{"execution":{"iopub.status.busy":"2022-08-16T11:02:46.895394Z","iopub.execute_input":"2022-08-16T11:02:46.896405Z","iopub.status.idle":"2022-08-16T11:02:47.141337Z","shell.execute_reply.started":"2022-08-16T11:02:46.896367Z","shell.execute_reply":"2022-08-16T11:02:47.140186Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"lstm_model4_hist = lstm_model4.fit(x = x_train,\n                                   y = y_train,\n                                   validation_data = (x_val, y_val),\n                                   epochs = 10,\n                                   verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T11:03:07.944886Z","iopub.execute_input":"2022-08-16T11:03:07.945366Z","iopub.status.idle":"2022-08-16T11:03:24.436147Z","shell.execute_reply.started":"2022-08-16T11:03:07.945331Z","shell.execute_reply":"2022-08-16T11:03:24.435225Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"metrics_plot(lstm_model4_hist.history)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T11:03:27.534211Z","iopub.execute_input":"2022-08-16T11:03:27.534579Z","iopub.status.idle":"2022-08-16T11:03:28.018132Z","shell.execute_reply.started":"2022-08-16T11:03:27.534547Z","shell.execute_reply":"2022-08-16T11:03:28.017123Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"lstm_model5 = Sequential([\n    layers.Embedding(vocab_size + 1,64),\n    Dropout(0.2),\n    layers.LSTM(64),\n    Dense(23, activation = 'relu'),\n    Dense(1, activation = 'sigmoid')\n])\n\nlstm_model5.compile(optimizer = Adam(learning_rate=0.0001),\n                    loss=\"binary_crossentropy\", \n                    metrics = ['accuracy', tf.keras.metrics.AUC()])\n\nlstm_model5.summary()\n\nlstm_model5_hist = lstm_model5.fit(x = x_train,\n                                   y = y_train,\n                                   validation_data = (x_val, y_val),\n                                   epochs = 10,\n                                   verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T11:07:49.616858Z","iopub.execute_input":"2022-08-16T11:07:49.617246Z","iopub.status.idle":"2022-08-16T11:08:04.930443Z","shell.execute_reply.started":"2022-08-16T11:07:49.617213Z","shell.execute_reply":"2022-08-16T11:08:04.929474Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"metrics_plot(lstm_model5_hist.history)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T11:08:07.047765Z","iopub.execute_input":"2022-08-16T11:08:07.050831Z","iopub.status.idle":"2022-08-16T11:08:07.582270Z","shell.execute_reply.started":"2022-08-16T11:08:07.050784Z","shell.execute_reply":"2022-08-16T11:08:07.581353Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"So far these results have not been very interesting. There isn't much variation in results, but let's try to see if it's different if we make the lstm bidirectional.","metadata":{}},{"cell_type":"code","source":"bid_model1 = Sequential([\n    layers.Embedding(vocab_size + 1,128),\n    Dropout(0.2),\n    layers.Bidirectional(layers.LSTM(64)),\n    Dense(23, activation = 'relu'),\n    Dense(1, activation = 'sigmoid')\n])\n\nbid_model1.compile(optimizer = Adam(learning_rate=0.0001),\n                    loss=\"binary_crossentropy\", \n                    metrics = ['accuracy', tf.keras.metrics.AUC()])\n\nbid_model1.summary()\n\nbid_model1_hist = bid_model1.fit(x = x_train,\n                                   y = y_train,\n                                   validation_data = (x_val, y_val),\n                                   epochs = 10,\n                                 verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T11:08:34.422226Z","iopub.execute_input":"2022-08-16T11:08:34.422953Z","iopub.status.idle":"2022-08-16T11:08:53.884990Z","shell.execute_reply.started":"2022-08-16T11:08:34.422914Z","shell.execute_reply":"2022-08-16T11:08:53.883934Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"metrics_plot(bid_model1_hist.history)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T11:23:25.497023Z","iopub.execute_input":"2022-08-16T11:23:25.497729Z","iopub.status.idle":"2022-08-16T11:23:25.978843Z","shell.execute_reply.started":"2022-08-16T11:23:25.497691Z","shell.execute_reply":"2022-08-16T11:23:25.977934Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"bid_model2 = Sequential([\n    layers.Embedding(vocab_size + 1,128),\n    Dropout(0.5),\n    layers.Bidirectional(layers.LSTM(64)),\n    Dense(1, activation = 'sigmoid')\n])\n\nbid_model2.compile(optimizer = Adam(learning_rate=0.0001),\n                    loss=\"binary_crossentropy\", \n                    metrics = ['accuracy', tf.keras.metrics.AUC()])\n\nbid_model2.summary()\n\nbid_model2_hist = bid_model2.fit(x = x_train,\n                                   y = y_train,\n                                   validation_data = (x_val, y_val),\n                                   epochs = 10,\n                                 verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T11:11:12.941541Z","iopub.execute_input":"2022-08-16T11:11:12.941920Z","iopub.status.idle":"2022-08-16T11:11:36.563421Z","shell.execute_reply.started":"2022-08-16T11:11:12.941887Z","shell.execute_reply":"2022-08-16T11:11:36.562369Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"metrics_plot(bid_model2_hist.history)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T11:23:18.130183Z","iopub.execute_input":"2022-08-16T11:23:18.131133Z","iopub.status.idle":"2022-08-16T11:23:18.595837Z","shell.execute_reply.started":"2022-08-16T11:23:18.131096Z","shell.execute_reply":"2022-08-16T11:23:18.594102Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"bid_model3 = Sequential([\n    layers.Embedding(vocab_size + 1,128),\n    Dropout(0.5),\n    layers.Bidirectional(layers.LSTM(64)),\n    Dense(1, activation = 'sigmoid')\n])\n\nbid_model3.compile(optimizer = Adam(learning_rate=0.001),\n                    loss=\"binary_crossentropy\", \n                    metrics = ['accuracy', tf.keras.metrics.AUC()])\n\nbid_model3.summary()\n\nbid_model3_hist = bid_model3.fit(x = x_train,\n                                 y = y_train,\n                                 validation_data = (x_val, y_val),\n                                 epochs = 10,\n                                 verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T11:22:14.180327Z","iopub.execute_input":"2022-08-16T11:22:14.180819Z","iopub.status.idle":"2022-08-16T11:22:34.157101Z","shell.execute_reply.started":"2022-08-16T11:22:14.180777Z","shell.execute_reply":"2022-08-16T11:22:34.156182Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"metrics_plot(bid_model3_hist.history)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T11:23:08.964484Z","iopub.execute_input":"2022-08-16T11:23:08.964923Z","iopub.status.idle":"2022-08-16T11:23:09.555181Z","shell.execute_reply.started":"2022-08-16T11:23:08.964884Z","shell.execute_reply":"2022-08-16T11:23:09.554107Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"I ran every model for 10 epochs. This runs much much faster than the cancer image set. The first epoch was always slowest, but even then it was multitudes faster. Here are the results if I were to stop at the highest validation accuracy. Disappointingly, they didn't seem to vary all that much from 79% validation accuracy. The best performing model seems to be the last model, but that was also at the first epoch. The learning rates that were one magnitude lower at 0.0001 were a lot more stable at reaching around 0.79 than the higher magnitude of 0.001. Train accuracy usually got very high toward the end of the epochs, but validation accuracy scores rarely saw consistent improvement. Instead, it would usally drop off very early in the epochs and you can see this in the metric graphs for every model. This likely means that model overfits to the data and overfits it fast. There seems to be a very slight improvement in validation accuracy with the bidirection model when compared to the non bidirection lstms. With scores this close it's hard to tell which direction to head, especially with 8 models run with varying hyper parameters. It is notable that the bidirectional models had double the parameters usually because of how I was encoding the embedded. Maybe a diffirent architechture would do better.","metadata":{}},{"cell_type":"markdown","source":"| model | type | drop_out | learning rate | train_acc | val_acc | val_auc\n| ---- | ---- | ---- | ---- | --- | --- | --- | \n| lstm_model1 | 2 Hidden | 0.2 | 0.001 | 0.8713 | 0.7905 | 0.8514 |\n| lstm_model2 | 2 Hidden | 0.5 | 0.001 | 0.8997 | 0.7814 | 0.8478 |\n| lstm_model3 | 2 Hidden | 0.5 | 0.0001 | 0.8568 | 0.7866 | 0.8498 |\n| lstm_model4 | 2 Hidden | 0.2 | 0.0001 | 0.9112 | 0.7859 | 0.8495 |\n| lstm_model5 | 1 Hidden | 0.2 | 0.0001 | 0.9039 | 0.7919 | 0.850 |\n| bid_model1 | 1 Hidden | 0.2 | 0.0001 | 0.8363 | 0.7951 | 0.8540 |\n| bid_model2 | 1 Hidden | 0.5 | 0.0001 | 0.8747 | 0.7938 | 0.8580 |\n| bid_model3 | 1 Hidden | 0.5 | 0.001 | 0.7223 | 0.7965 | 0.8556 |","metadata":{}},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"code","source":"callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2, restore_best_weights=True)\nbid_model1 = Sequential([\n    layers.Embedding(vocab_size + 1,128),\n    Dropout(0.2),\n    layers.Bidirectional(layers.LSTM(64)),\n    Dense(23, activation = 'relu'),\n    Dense(1, activation = 'sigmoid')\n])\n\nbid_model1.compile(optimizer = Adam(learning_rate=0.0001),\n                    loss=\"binary_crossentropy\", \n                    metrics = ['accuracy', tf.keras.metrics.AUC()])\n\nbid_model1.summary()\n\nbid_model1_hist = bid_model1.fit(x = x_train,\n                                 y = y_train,\n                                 validation_data = (x_val, y_val),\n                                 epochs = 10,\n                                 callbacks=[callback],\n                                 verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T11:47:51.652606Z","iopub.execute_input":"2022-08-16T11:47:51.652988Z","iopub.status.idle":"2022-08-16T11:48:11.038185Z","shell.execute_reply.started":"2022-08-16T11:47:51.652957Z","shell.execute_reply":"2022-08-16T11:48:11.037293Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\ny_preds1 = bid_model1.predict(x_val, verbose=1)\ny_hat_labels1 = np.round(y_preds1)\nconf_mat1 = confusion_matrix(y_val, y_hat_labels1)/len(y_val)\nsns.heatmap(conf_mat1, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\")\nplt.title(\"Confusion Matrix\")","metadata":{"execution":{"iopub.status.busy":"2022-08-16T11:50:28.505040Z","iopub.execute_input":"2022-08-16T11:50:28.505648Z","iopub.status.idle":"2022-08-16T11:50:29.047263Z","shell.execute_reply.started":"2022-08-16T11:50:28.505611Z","shell.execute_reply":"2022-08-16T11:50:29.046112Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"I reran with early stopping for the last model which was bidirectional, since it performed best, although the difference was very slight. I had it restore the weights to the epoch that had the best validation accuracy scores. It stopped at a slightly higher validation and managed to break 0.8, but these small increases are random and aren't really good indicators. From the confusion matrix using this model, you can see that this model has a harder time predicting true positives as there seems to be a quite a lot more false negatives than false positives, nearly double the amount. It's likely that this model is bias toward predicting not disaster when it is a disaster. To improve, we need to try different architectures and perhaps try to think of a way that also includes data about the keywords.","metadata":{}},{"cell_type":"markdown","source":"# Create Submission","metadata":{}},{"cell_type":"code","source":"test_df_clean = preprocessPipline(test_df, 'text')\n\ntest_df_clean = tokenizer.texts_to_sequences(test_df_clean['text'])\n\ntest_pad = pad_sequences(test_df_clean,\n                                maxlen=23, \n                                truncating='post', \n                                padding='post'\n                               )","metadata":{"execution":{"iopub.status.busy":"2022-08-16T11:55:58.752288Z","iopub.execute_input":"2022-08-16T11:55:58.752661Z","iopub.status.idle":"2022-08-16T11:55:59.085161Z","shell.execute_reply.started":"2022-08-16T11:55:58.752632Z","shell.execute_reply":"2022-08-16T11:55:59.084199Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"test_y = bid_model1.predict(test_pad, verbose=1)\ntest_y = np.round(test_y)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T11:56:03.464745Z","iopub.execute_input":"2022-08-16T11:56:03.465311Z","iopub.status.idle":"2022-08-16T11:56:03.906969Z","shell.execute_reply.started":"2022-08-16T11:56:03.465276Z","shell.execute_reply":"2022-08-16T11:56:03.905923Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\nsubmission['target']=test_y.astype('int')\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-16T11:56:05.720731Z","iopub.execute_input":"2022-08-16T11:56:05.721092Z","iopub.status.idle":"2022-08-16T11:56:05.738191Z","shell.execute_reply.started":"2022-08-16T11:56:05.721060Z","shell.execute_reply":"2022-08-16T11:56:05.737196Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T11:56:44.950551Z","iopub.execute_input":"2022-08-16T11:56:44.951185Z","iopub.status.idle":"2022-08-16T11:56:44.970216Z","shell.execute_reply.started":"2022-08-16T11:56:44.951122Z","shell.execute_reply":"2022-08-16T11:56:44.968941Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"# References:\n\nhttps://www.kaggle.com/code/mariapushkareva/nlp-disaster-tweets-with-glove-and-lstm/notebook\n\nhttps://www.kaggle.com/code/andreshg/nlp-glove-bert-tf-idf-lstm-explained\n\nhttps://www.kaggle.com/code/alid3bs/lstm-vs-gru-vs-bidirectional/notebook\n\nhttps://www.kaggle.com/code/danilastepochkin/disaster-tweets-dl-with-lstm-and-language-model\n\nhttps://www.kaggle.com/code/ltrahul/nlp-disaster-tweets-prediction-with-nltk-and-lstm","metadata":{}}]}